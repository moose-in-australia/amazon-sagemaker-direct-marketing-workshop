{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Amazon SageMaker Workshop - Direct Marketing with XGBoost\n",
    "_**A basic Amazon SageMaker workshop demonstrating the machine learning workflow from data preparation to model deployment using direct marketing data.**_\n",
    "\n",
    "The code is based on the [XGBoost Direct Marketing example](https://github.com/awslabs/amazon-sagemaker-examples/tree/master/hyperparameter_tuning/xgboost_direct_marketing) for Amazon SageMaker.\n",
    "\n",
    "<div class=\"alert alert-block alert-info\">\n",
    "    <b>Note:</b> You may be asked to choose a kernel for this notebook. Please use the following:    \n",
    "    <ul><li>Standard Amazon SageMaker notebook instances - <i>conda_python3</i></li>\n",
    "        <li>Amazon SageMaker Studio notebooks - <i>Python 3 (Data Science)</i></li></ul>\n",
    "</div>\n",
    "\n",
    "## Contents\n",
    "\n",
    "1. [Introduction](#intro)\n",
    "1. [Setup](#setup)\n",
    "1. [Data preparation](#data-prep)\n",
    "1. [Training and hyperparameter tuning](#hyperparam1)\n",
    "1. [Evaluate model](#eval1)\n",
    "1. [Deploy](#deploy)\n",
    "1. [Conclusion](#conclusion)\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "## Introduction <a class=\"anchor\" id=\"intro\"></a>\n",
    "\n",
    "Amazon SageMaker is a fully-managed service that covers the entire machine learning workflow to label and prepare your data, choose an algorithm, train the model, tune and optimize it for deployment, make predictions, and take action. In this workshop, you will run through the process end-to-end to understand how SageMaker helps you develop models faster from concept to production.\n",
    "\n",
    "The process of building models in Amazon SageMaker can be divided into four stages:\n",
    "1. **Ground Truth**: Label your data for supervised learning tasks. Set up data labeling jobs using public or private teams. Ground Truth takes care of dividing the data between the annotators and consolidating the results. \n",
    "1. **Notebook**: Use Jupyter notebooks to explore and process your data. This is your environment for experimentation. Notebooks run on fully managed EC2 servers.\n",
    "1. **Training**: Run training and hyperparameter tuning jobs at any scale. The underlying resources are automatically spun up as required, and automatically spun down when the job is finished. \n",
    "1. **Inference**: Deploy your model in the real world. Make it available 24/7 through inference endpoints, or process data in batches.\n",
    "\n",
    "<img src=\"img/sagemaker_process.PNG\" alt=\"The SageMaker process\" width=\"800\"/>\n",
    "\n",
    "This notebook will train a model which can be used to predict if a customer will enroll for a term deposit at a bank, after one or more phone calls, based on data from the [Bank Marketing data set](https://archive.ics.uci.edu/ml/datasets/bank+marketing). You will work through the steps of the common machine learning process depicted below, with the exception of data labeling, since annotated data is already available. Although the process of developing a good model is iterative, this notebook will only cover one iteration of model creation and deployment.\n",
    "\n",
    "<img src=\"img/ml_process.png\" alt=\"The SageMaker process\" width=\"600\"/>\n",
    "\n",
    "---\n",
    "\n",
    "## Setup <a class=\"anchor\" id=\"setup\"></a>\n",
    "\n",
    "First, set up some of the basic resources required to run SageMaker. This includes:\n",
    "\n",
    "- The S3 bucket and prefix that you want to use for your training and model data. This should be within the same region as your SageMaker notebook instance.\n",
    "- The IAM role used to give training access to your data.\n",
    "\n",
    "<div class=\"alert alert-block alert-info\">\n",
    "    <b>Note:</b> If you used the CloudFormation template to create the resources for this workshop in your account, or if you are running this notebook as part of an AWS-hosted workshop, an S3 bucket has already been created in your account. Identify the right S3 bucket and copy/paste this name in the code below.\n",
    "\n",
    "If you do not have an existing S3 bucket in your account for this workshop, use `bucket = sagemaker_session.default_bucket()` to have SageMaker create a bucket for you.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sagemaker\n",
    "import boto3\n",
    "\n",
    "# Get the role associated with this SageMaker notebook.\n",
    "role = sagemaker.get_execution_role()\n",
    "print(\"Role name: {}\".format(role))\n",
    "\n",
    "# Start a session\n",
    "sagemaker_session = sagemaker.Session()\n",
    "\n",
    "# Specify an S3 bucket for storing the training data.\n",
    "# !ACTION REQUIRED! Replace <TODO> with the name of the S3 bucket created by the CloudFormation template.\n",
    "# If no S3 bucket has been created, use bucket = sagemaker_session.default_bucket()\n",
    "bucket = '<TODO>'\n",
    "print(\"Bucket name: {}\".format(bucket))\n",
    "\n",
    "# Set a prefix for storing your data - this will look like a folder in the S3 bucket.\n",
    "prefix = 'sagemaker-workshop-marketing'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data preparation <a class=\"anchor\" id=\"data-prep\"></a>\n",
    "Start by downloading the [Bank Marketing data set](https://archive.ics.uci.edu/ml/datasets/bank+marketing) from UCI's ML Repository. Then read in the data and display it as a table. It contains records of direct marketing campaigns, such that each row is a phone call made to a customer. The goal is to predict if the customer will subscribe a term deposit, which is represented by the target variable `y`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download and unzip the marketing data set.\n",
    "!wget -N https://sagemaker-sample-data-us-west-2.s3-us-west-2.amazonaws.com/autopilot/direct_marketing/bank-additional.zip\n",
    "path_to_zip_file = 'bank-additional.zip'\n",
    "directory_to_extract_to = '.'\n",
    "\n",
    "import zipfile\n",
    "with zipfile.ZipFile(path_to_zip_file, 'r') as zip_ref:\n",
    "    zip_ref.extractall(directory_to_extract_to)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Read and display the marketing data set.\n",
    "data = pd.read_csv('./bank-additional/bank-additional-full.csv', sep=',')\n",
    "pd.set_option('display.max_columns', 500)     # Make sure we can see all of the columns\n",
    "pd.set_option('display.max_rows', 50)         # Keep the output on one page\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a look at the data.  At a high level, we can see:\n",
    "\n",
    "* We have a little over 40K customer records, and 20 features for each customer.\n",
    "* The features are mixed; some numeric, some categorical.\n",
    "* The data appears to be sorted, at least by `time` and `contact`.\n",
    "\n",
    "_**Specifics on each of the features:**_\n",
    "\n",
    "*Demographics:*\n",
    "* `age`: Customer's age (numeric)\n",
    "* `job`: Type of job (categorical: 'admin.', 'services', ...)\n",
    "* `marital`: Marital status (categorical: 'married', 'single', ...)\n",
    "* `education`: Level of education (categorical: 'basic.4y', 'high.school', ...)\n",
    "\n",
    "*Past customer events:*\n",
    "* `default`: Has credit in default? (categorical: 'no', 'unknown', ...)\n",
    "* `housing`: Has housing loan? (categorical: 'no', 'yes', ...)\n",
    "* `loan`: Has personal loan? (categorical: 'no', 'yes', ...)\n",
    "\n",
    "*Past direct marketing contacts:*\n",
    "* `contact`: Contact communication type (categorical: 'cellular', 'telephone', ...)\n",
    "* `month`: Last contact month of year (categorical: 'may', 'nov', ...)\n",
    "* `day_of_week`: Last contact day of the week (categorical: 'mon', 'fri', ...)\n",
    "* `duration`: Last contact duration, in seconds (numeric). Important note: If duration = 0 then `y` = 'no'.\n",
    " \n",
    "*Campaign information:*\n",
    "* `campaign`: Number of contacts performed during this campaign and for this client (numeric, includes last contact)\n",
    "* `pdays`: Number of days that passed by after the client was last contacted from a previous campaign (numeric)\n",
    "* `previous`: Number of contacts performed before this campaign and for this client (numeric)\n",
    "* `poutcome`: Outcome of the previous marketing campaign (categorical: 'nonexistent','success', ...)\n",
    "\n",
    "*External environment factors:*\n",
    "* `emp.var.rate`: Employment variation rate - quarterly indicator (numeric)\n",
    "* `cons.price.idx`: Consumer price index - monthly indicator (numeric)\n",
    "* `cons.conf.idx`: Consumer confidence index - monthly indicator (numeric)\n",
    "* `euribor3m`: Euribor 3 month rate - daily indicator (numeric)\n",
    "* `nr.employed`: Number of employees - quarterly indicator (numeric)\n",
    "\n",
    "*Target variable:*\n",
    "* `y`: Has the client subscribed a term deposit? (binary: 'yes','no')\n",
    "\n",
    "Let's start exploring the data. First, let's understand how the features are distributed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Frequency tables for each categorical feature\n",
    "for column in data.select_dtypes(include=['object']).columns:\n",
    "    display(pd.crosstab(index=data[column], columns='% observations', normalize='columns'))\n",
    "\n",
    "# Histograms for each numeric features\n",
    "display(data.describe())\n",
    "%matplotlib inline\n",
    "hist = data.hist(bins=30, sharey=True, figsize=(10, 10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that:\n",
    "\n",
    "* Almost 90% of the values for our target variable y are \"no\", so most customers did not subscribe to a term deposit.\n",
    "* Many of the predictive features take on values of \"unknown\". Some are more common than others. We should think carefully as to what causes a value of \"unknown\" (are these customers non-representative in some way?) and how we that should be handled.\n",
    "    * Even if \"unknown\" is included as it's own distinct category, what does it mean given that, in reality, those observations likely fall within one of the other categories of that feature?\n",
    "* Many of the predictive features have categories with very few observations in them. If we find a small category to be highly predictive of our target outcome, do we have enough evidence to make a generalization about that?\n",
    "* Contact timing is particularly skewed. Almost a third in May and less than 1% in December. What does this mean for predicting our target variable next December?\n",
    "* There are no missing values in our numeric features. Or missing values have already been imputed.\n",
    "    * pdays takes a value near 1000 for almost all customers. Likely a placeholder value signifying no previous contact.\n",
    "* Several numeric features have a very long tail. Do we need to handle these few observations with extremely large values differently?\n",
    "* Several numeric features (particularly the macroeconomic ones) occur in distinct buckets. Should these be treated as categorical?\n",
    "\n",
    "Next, let's look at how our features relate to the target that we are attempting to predict."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "for column in data.select_dtypes(include=['object']).columns:\n",
    "    if column != 'y':\n",
    "        display(pd.crosstab(index=data[column], columns=data['y'], normalize='columns'))\n",
    "\n",
    "for column in data.select_dtypes(exclude=['object']).columns:\n",
    "    print(column)\n",
    "    hist = data[[column, 'y']].hist(by='y', bins=30)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that:\n",
    "\n",
    "* Customers who are-- \"blue-collar\", \"married\", \"unknown\" default status, contacted by \"telephone\", and/or in \"may\" are a substantially lower portion of \"yes\" than \"no\" for subscribing.\n",
    "* Distributions for numeric variables are different across \"yes\" and \"no\" subscribing groups, but the relationships may not be straightforward or obvious.\n",
    "\n",
    "Now let's look at how our features relate to one another."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(data.corr())\n",
    "pd.plotting.scatter_matrix(data, figsize=(12, 12))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that:\n",
    "* Features vary widely in their relationship with one another. Some with highly negative correlation, others with highly positive correlation.\n",
    "* Relationships between features is non-linear and discrete in many cases.\n",
    "\n",
    "Normally, you won't be able to use the data as-is for training machine learning models. Every algorithm has specific requirements on the input it accepts, so you will need to transfrom the data based on these requirements. In addition, your data set will often contain incomplete data, duplicate data, inconsistent data, irrelevant data, or outliers. These problems will affect the performance of the model, so data cleaning is a crucial step in the machine learning process. Several common techniques include:\n",
    "\n",
    "* Handling missing values: Some machine learning algorithms are capable of handling missing values, but most would rather not. Options include:\n",
    "    * Removing observations with missing values: This works well if only a very small fraction of observations have incomplete information.\n",
    "    * Removing features with missing values: This works well if there are a small number of features which have a large number of missing values.\n",
    "    * Imputing missing values: Entire books have been written on this topic, but common choices are replacing the missing value with the mode or mean of that column's non-missing values.\n",
    "* Converting categorical to numeric: The most common method is one hot encoding, which for each feature maps every distinct value of that column to its own feature which takes a value of 1 when the categorical feature is equal to that value, and 0 otherwise.\n",
    "* Oddly distributed data: Although for non-linear models like Gradient Boosted Trees, this has very limited implications, parametric models like regression can produce wildly inaccurate estimates when fed highly skewed data. In some cases, simply taking the natural log of the features is sufficient to produce more normally distributed data. In others, bucketing values into discrete ranges is helpful. These buckets can then be treated as categorical variables and included in the model when one hot encoded.\n",
    "* Handling more complicated data types: Mainpulating images, text, or data at varying grains is left for other notebook templates.\n",
    "\n",
    "To get started, one question to ask yourself is whether certain features will add value in your final use case. For example, if your goal is to deliver the best prediction, then will you have access to that data at the moment of prediction? Knowing it's raining is highly predictive for umbrella sales, but forecasting weather far enough out to plan inventory on umbrellas is probably just as difficult as forecasting umbrella sales without knowledge of the weather. So, including this in your model may give you a false sense of precision. Following this logic, let's remove the economic features and duration from our data as they would need to be forecasted with high precision to use as inputs in future predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop columns requiring accurate future forecasts\n",
    "clean_data = data.drop(['duration', 'emp.var.rate', 'cons.price.idx', 'cons.conf.idx', 'euribor3m', 'nr.employed'], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The cell below has been left blank for you to apply your own data cleaning steps. Below are some ideas if you are not sure what to do:\n",
    "1. Many records have the value of \"999\" for pdays, number of days that passed by after a client was last contacted. It is very likely to be a magic number to represent that no contact was made before. Considering this, we can create a new column called \"no_previous_contact\", then grant it value of \"1\" when pdays is 999 and \"0\" otherwise.\n",
    "1. In the \"job\" column, there are categories that mean the customer is not working, e.g., \"student\", \"retire\", and \"unemployed\". Since it is very likely whether or not a customer is working will affect his/her decision to enroll in the term deposit, we can generate a new column to show whether the customer is working based on \"job\" column.\n",
    "\n",
    "Alternatively, you can choose to continue without additional data cleaning. After training and evaluating a model based on the current data set, you can iterate on this by coming back to this data cleaning step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !ACTION REQUIRED!\n",
    "# Apply your own data cleaning steps below on the clean_data variable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this workshop, you will use the [XGBoost algorithm](https://docs.aws.amazon.com/sagemaker/latest/dg/xgboost.html) to predict the likelihood of a customer enrolling for a term deposit. More information about this algorithm will be provided in the training and tuning section, but for data preparation it is important to know the input requirements of the algorithm. One of the data formats accepted by XGBoost is CSV (comma-separated value). The CSV should be structured such that the rows are observations, the first column is the target variable, and the remaining columns are the features. In addition, the CSV file should contain only numeric data and no header. For more details, [read the documentation](https://docs.aws.amazon.com/sagemaker/latest/dg/xgboost.html#InputOutput-XGBoost) on the input and output format of XGBoost. If the data is not formatted correctly, you can expect to see an error similar to the image below.\n",
    "\n",
    "<img src=\"img/workbook_screenshot2.PNG\" alt=\"Error message for incorrect data input.\" width=\"600\"/>\n",
    "\n",
    "First, let's ensure that the data only contains numeric values. This means converting all categorical variables (e.g. `marital` column) into sets of indicators. A new column is created for each value (e.g. marital_divorced, marital_married, marital_single, and marital_unknown), and for each user (i.e. row) a 1 marks the relevant value while the rest of the columns contain 0's. This method is known as one-hot encoding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert categorical variables to sets of indicators\n",
    "model_data = pd.get_dummies(clean_data)\n",
    "model_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since our target variable was categorical (i.e. 'yes' or 'no'), the one-hot encoding method resulted in two columns: `y_yes` and `y_no`, which are exact opposites of each other. XGBoost only accepts one target variable column and expects that to be the first column in the data set, so the data set needs to be transformed to reflect these requirements.\n",
    "\n",
    "When building a model whose primary goal is to predict a target value on new data, it is important to understand overfitting. Supervised learning models are designed to minimize error between their predictions of the target value and actuals, in the data they are given. This last part is key, as frequently in their quest for greater accuracy, machine learning models bias themselves toward picking up on minor idiosyncrasies within the data they are shown. These idiosyncrasies then don't repeat themselves in subsequent data, meaning those predictions can actually be made less accurate, at the expense of more accurate predictions in the training phase.\n",
    "\n",
    "The most common way of preventing this is to build models with the concept that a model shouldn't only be judged on its fit to the data it was trained on, but also on \"new\" data. There are several different ways of operationalizing this, holdout validation, cross-validation, leave-one-out validation, etc. For our purposes, we'll simply randomly split the data into 3 uneven groups. The model will be trained on 70% of data, it will then be evaluated on 20% of data to give us an estimate of the accuracy we hope to have on \"new\" data, and 10% will be held back as a final testing dataset which will be used later on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove excess target variable column and copy the remaining target variable column to be the first column\n",
    "model_data = pd.concat([model_data['y_yes'], model_data.drop(['y_no', 'y_yes'], axis=1)], axis=1)\n",
    "\n",
    "# Randomly sort the data then split out first 70%, second 20%, and last 10%\n",
    "train_data, validation_data, test_data = np.split(model_data.sample(frac=1, random_state=1729), \n",
    "                                                  [int(0.7 * len(model_data)), \n",
    "                                                   int(0.9 * len(model_data))])\n",
    "\n",
    "# Save to CSV format. Drop headers and index column.\n",
    "train_data.to_csv('train.csv', index=False, header=False)\n",
    "validation_data.to_csv('validation.csv', index=False, header=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To build a SageMaker model, the training and validation data need to be stored in an S3 bucket. Remember that the bucket should be in the same AWS region as your SageMaker instance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the file locations\n",
    "s3train = 's3://{}/{}/train/'.format(bucket, prefix)\n",
    "s3validation = 's3://{}/{}/validation/'.format(bucket, prefix)\n",
    "\n",
    "# Upload the data to S3\n",
    "!aws s3 cp train.csv $s3train\n",
    "!aws s3 cp validation.csv $s3validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After uploading the data to S3, we need to tell SageMaker where to find the data and where to save the output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tell SageMaker where to find the input data and where to store the output\n",
    "s3_output_location = 's3://{}/{}/output'.format(bucket, prefix)\n",
    "s3_input_train = sagemaker.inputs.TrainingInput(s3train, distribution='FullyReplicated', \n",
    "                        content_type='text/csv', s3_data_type='S3Prefix')\n",
    "s3_input_validation = sagemaker.inputs.TrainingInput(s3validation, distribution='FullyReplicated', \n",
    "                             content_type='text/csv', s3_data_type='S3Prefix')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training and hyperparameter tuning <a class=\"anchor\" id=\"hyperparam1\"></a>\n",
    "\n",
    "In this workshop, you will use XGBoost, also known as Extreme Gradient Boosting. XGBoost is a built-in algorithm in SageMaker which can be used for regression and classification on structured (tabular) data. It works by building an ensemble of decision trees. During training, decision trees are continuously added to predict and correct the errors of the previous decision trees, until no further improvements can be made (i.e. boosting). This algorithm has been shown to work well with minimal data cleaning, performing well despite missing data points or outliers. With the large number of hyperparameters contained in this algorithm, tuning the model is crucial to achieving success.\n",
    "\n",
    "First, you need to specify how a training job should be run:\n",
    "* Provide the container image for the algorithm (XGBoost)\n",
    "* The location where the output should be saved (S3 bucket)\n",
    "* The values of any algorithm hyperparameters that are not tuned in the tuning job (static hyperparameters)\n",
    "* The type and number of instances to use for the training jobs\n",
    "* The stopping condition for the training jobs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fetch the XGBoost container for your region\n",
    "my_region = sagemaker_session.boto_region_name\n",
    "container = sagemaker.image_uris.retrieve('xgboost', my_region, '0.90-2')\n",
    "\n",
    "# Set up an XGBoost trainer\n",
    "xgb = sagemaker.estimator.Estimator(container,\n",
    "                                    role, \n",
    "                                    instance_count=1, \n",
    "                                    instance_type='ml.m4.xlarge',\n",
    "                                    output_path=s3_output_location,\n",
    "                                    sagemaker_session=sagemaker_session)\n",
    "\n",
    "# Set initial values for all hyperparameters\n",
    "xgb.set_hyperparameters(max_depth=5,\n",
    "                        eta=0.2,\n",
    "                        gamma=4,\n",
    "                        min_child_weight=6,\n",
    "                        subsample=0.8,\n",
    "                        silent=0,\n",
    "                        objective='binary:logistic',\n",
    "                        num_round=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With just these settings from the cell above, you could go ahead and train a model. However, XGBoost has a large number of hyperparameters, so it is unlikely that we chose the best values above.\n",
    "\n",
    "SageMaker automatic model tuning helps with automating the hyperparameter tuning process. It allows you to define a range for each hyperparameter, and SageMaker will automatically run multiple training jobs with different hyperparameter values, evaluating the results based on a predefined objective metric. Results from the previous tuning jobs are used to choose values for consecutive tuning jobs. You can set a tuning budget (maximum number of training jobs), to stop the tuning process. Often the tuning process is iterative and requires running multiple tuning jobs after analyzing the results to get the best objective metric.\n",
    "\n",
    "Now we configure the hyperparameter tuning job by defining a JSON object that specifies following information:\n",
    "* The ranges of hyperparameters we want to tune\n",
    "* Number of training jobs to run in total and how many training jobs should be run simultaneously. More parallel jobs will finish tuning sooner, but may sacrifice accuracy. We recommend you set the parallel jobs value to less than 10% of the total number of training jobs (we'll set it higher just for this example to keep it short).\n",
    "* The objective metric that will be used to evaluate training results, in this example, we select *validation:auc* to be the objective metric and the goal is to maximize the value throughout the hyperparameter tuning process. One thing to note is the objective metric has to be among the metrics that are emitted by the algorithm during training. In this example, the built-in XGBoost algorithm emits a bunch of metrics and *validation:auc* is one of them. If you bring your own algorithm to SageMaker, then you need to make sure that it emits the objective metric you select.\n",
    "\n",
    "We will tune four hyperparameters in this examples:\n",
    "* *eta*: Step size shrinkage used in updates to prevent overfitting. After each boosting step, you can directly get the weights of new features. The eta parameter actually shrinks the feature weights to make the boosting process more conservative. \n",
    "* *alpha*: L1 regularization term on weights. Increasing this value makes models more conservative. \n",
    "* *min_child_weight*: Minimum sum of instance weight (hessian) needed in a child. If the tree partition step results in a leaf node with the sum of instance weight less than min_child_weight, the building process gives up further partitioning. In linear regression models, this simply corresponds to a minimum number of instances needed in each node. The larger the algorithm, the more conservative it is. \n",
    "* *max_depth*: Maximum depth of a tree. Increasing this value makes the model more complex and likely to be overfitted. \n",
    "\n",
    "In the following cell you are required to enter some code following the instructions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.tuner import IntegerParameter, ContinuousParameter, HyperparameterTuner\n",
    "\n",
    "# !ACTION REQUIRED! In the code below, you need to replace the <TODO>'s!\n",
    "\n",
    "# Set the search range for each hyperparameter\n",
    "# This is a dictionary with 4 members defining the ranges of the hyperparameters to search:\n",
    "# 'eta' must be set as a continuous parameter ranging from 0 to 1\n",
    "# 'alpha' must be set as a continuous parameter ranging from 0 to 2\n",
    "# 'min_child_weight' must be set as a continuous parameter ranging from 1 to 10\n",
    "# 'max_depth' must be set as an integer parameter raging from 1 to 10\n",
    "hyperparameter_ranges = {'eta': ContinuousParameter(0, 1),\n",
    "                         'alpha': <TODO>,\n",
    "                         'min_child_weight': <TODO>,\n",
    "                         'max_depth': <TODO>}\n",
    "\n",
    "# Choose the objective metric to maximize and set up the tuner\n",
    "objective_metric_name = 'validation:auc'\n",
    "# Create the HyperparameterTuner with the following arguments:\n",
    "# xgb - the estimator we created previously,\n",
    "# the objective_metric_name\n",
    "# an objective_type = 'Maximize'\n",
    "# 5 max_jobs\n",
    "# 3 max_parallel_jobs\n",
    "tuner = HyperparameterTuner(xgb,\n",
    "                            objective_metric_name,\n",
    "                            hyperparameter_ranges,\n",
    "                            objective_type='Maximize',\n",
    "                            max_jobs=5,\n",
    "                            max_parallel_jobs=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Time to launch the hyperparameter tuning job! You need to point the tuner to the training and validation sets you prepared earlier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tuner.fit({'train': s3_input_train, 'validation': s3_input_validation}, include_cls_metadata=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the SageMaker console, you can navigate to *Hyperparameter tuning jobs* to see the job in progress. In the column *Training completed / total*, you should at first see the value 0/3. SageMaker starts with 3 parallel tuning jobs because that is what we specified as the maximum parallel jobs. As the first three tuning jobs finish, you will see SageMaker start two more tuning jobs, for a total of 5 jobs, as we specified. Click on the name of your tuning job to see more details. \n",
    "\n",
    "<img src=\"img/tuning.PNG\" width=\"800\" />\n",
    "\n",
    "You can also navigate to *Training jobs* to view the individual training jobs started by the hyperparamater tuner.\n",
    "\n",
    "<img src=\"img/training.PNG\" width=\"800\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate model <a class=\"anchor\" id=\"eval1\"></a>\n",
    "\n",
    "<div class=\"alert alert-block alert-info\"><b>Note:</b> You will be unable to successfully run this section until the tuning job completes. Check the progress of the tuning job in the console, or run the first cell below to receive the status. While waiting, feel free to experiment with the data some more and apply additional cleaning.</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this cell to check current status of hyperparameter tuning job\n",
    "sage_client = sagemaker_session.sagemaker_client\n",
    "job_name = tuner._current_job_name\n",
    "tuning_job_result = sage_client.describe_hyper_parameter_tuning_job(HyperParameterTuningJobName=job_name)\n",
    "\n",
    "status = tuning_job_result['HyperParameterTuningJobStatus']\n",
    "print(\"Current status: {}\".format(status))\n",
    "if status != 'Completed':\n",
    "    print('Reminder: the tuning job has not been completed.')\n",
    "job_count = tuning_job_result['TrainingJobStatusCounters']['Completed']\n",
    "print(\"{} training jobs have completed\".format(job_count))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once the tuning job finishes, we can bring in a table of metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tuning_job_name = tuner._current_job_name\n",
    "\n",
    "tuner_parent_metrics = sagemaker.HyperparameterTuningJobAnalytics(tuning_job_name)\n",
    "if not tuner_parent_metrics.dataframe().empty:\n",
    "    df_parent = tuner_parent_metrics.dataframe().sort_values(['FinalObjectiveValue'], ascending=False)\n",
    "    \n",
    "df_parent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you are not interested in the full list of results, but just want to know which training job was the best, use \n",
    "'BestTrainingJob' from the Automatic Model Tuning describe API."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_overall_training_job = sage_client.describe_hyper_parameter_tuning_job(\n",
    "    HyperParameterTuningJobName=tuning_job_name)['BestTrainingJob']\n",
    "\n",
    "best_overall_training_job"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can analyze the results deeper by using [HPO_Analyze_TuningJob_Results.ipynb notebook](https://github.com/awslabs/amazon-sagemaker-examples/tree/master/hyperparameter_tuning/analyze_results). Here, we will just plot how the objective metric changes overtime as the tuning progresses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import bokeh\n",
    "import bokeh.io\n",
    "bokeh.io.output_notebook()\n",
    "from bokeh.plotting import figure, show\n",
    "from bokeh.models import HoverTool\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "df_parent_objective_value = df_parent[df_parent['FinalObjectiveValue'] > -float('inf')]\n",
    "\n",
    "p = figure(plot_width=900, plot_height=400, x_axis_type='datetime',x_axis_label='datetime', y_axis_label=objective_metric_name)\n",
    "p.circle(source=df_parent_objective_value, x='TrainingStartTime', y='FinalObjectiveValue', color='black')\n",
    "\n",
    "show(p)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Depending on how your first hyperparameter tuning job went, you may or may not want to try another tuning job to see whether the model quality can be further improved. When you decide to run another tuning job, you would want to leverage what has been known about the search space from the completed tuning job. In that case, you can create a new hyperparameter tuning job using a [warm start](https://docs.aws.amazon.com/sagemaker/latest/dg/automatic-model-tuning-warm-start.html). We won't cover this feature in this workshop, but it is good to know that it exists."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deploy <a class=\"anchor\" id=\"deploy\"></a>\n",
    "\n",
    "There are two ways to deploy your models and get predictions in Amazon SageMaker:\n",
    "* Endpoints: This deploys a persistent HTTPS endpoint containing one or more models. Use it to send individual inference requests which require near real-time responses.\n",
    "* Batch Transform: This spins up the resources required to run inference on a batch of data, and automatically shuts down the resources when finished. Use this to process large datasets or to run inference requests which do not require an immediate response. \n",
    "\n",
    "This notebook contains example code for both batch transform and endpoint deployment.\n",
    "\n",
    "### Batch Transform\n",
    "\n",
    "Let's test our best model by running a batch transform job. The advantage of running a batch transform job is that you only pay for the compute resources required to complete the inference request - Amazon SageMaker takes care of starting up and shutting down the resources.\n",
    "\n",
    "From the hyperparameter tuning job run in the previous section, we fetch only the best performing model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_xgb = tuner.best_estimator()\n",
    "best_xgb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see the specific hyperparameter values of this best performing model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_xgb.hyperparameters()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To run batch transforms using this model, we need to load it into a [Transformer](https://sagemaker.readthedocs.io/en/stable/transformer.html). You can specify the type and number of compute resources (GPU is usually not required for inference), as well as various parameters on the input and output format of the data. Batch transform allows you to filter the input data, and associate input records with inference results. We don't cover those features in this workshop, but it is worth reading the [documentation](https://docs.aws.amazon.com/sagemaker/latest/dg/batch-transform.html). \n",
    "\n",
    "For now, let's create a transformer on the smallest instance type. Remember that creating the transformer does not start up the compute resource - the instance will only be started up when a transform job is requested."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transformer = best_xgb.transformer(\n",
    "    instance_count = 1,\n",
    "    instance_type = 'ml.m4.xlarge',\n",
    "    strategy = 'SingleRecord',\n",
    "    output_path = s3_output_location,\n",
    "    assemble_with = 'Line',\n",
    ")\n",
    "transformer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remember that we split off 10% from our dataset into a test set at the start of this workshop?\n",
    "\n",
    "First remove the target variable column from this dataset - this will be compared with the predictions produced by the model later. Then apply the same processing used for the train and validation sets: save the data in CSV format and upload it to the S3 bucket."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop the target variable column from the dataset.\n",
    "test_data_wo_target = test_data.drop(['y_yes'], axis=1)\n",
    "\n",
    "# Save to CSV format. Drop headers and index column.\n",
    "test_data_wo_target.to_csv('test.csv', index=False, header=False)\n",
    "\n",
    "# Set the file locations\n",
    "s3test = 's3://{}/{}/test/'.format(bucket, prefix)\n",
    "\n",
    "# Upload the data to S3\n",
    "!aws s3 cp test.csv $s3test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now you are ready to run a batch transform on your test data! This will take around 5 minutes to complete - it needs to create the compute resources and run the whole dataset through the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transformer.transform(s3test, content_type=\"text/csv\", split_type=\"Line\")\n",
    "transformer.wait()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As soon as you start the batch transform job, you should see it appear in the 'Batch transform jobs' section of the SageMaker console.\n",
    "\n",
    "<img src=\"img/batch_transform.PNG\" width=\"800\" />\n",
    "\n",
    "Batch transform saves the results as a CSV file in the S3 bucket. This is great in production, but we want to take a look at the results in this notebook, so we copy the file onto the instance running this notebook. Then we load the data from the file and apply some transformations to get a 1-dimensional array of values.\n",
    "\n",
    "Using the transformed prediction results, we can create a confusion matrix to compare the model's predictions with the ground truth data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copy the output file from S3 to the instance underlying this notebook\n",
    "!aws s3 cp --recursive $transformer.output_path ./"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = pd.read_csv('test.csv.out', header=None)\n",
    "\n",
    "# Transpose the matrix, convert to a numpy array, then select only the first column\n",
    "predictions = predictions.T.to_numpy()[0]\n",
    "predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate a confusion matrix\n",
    "pd.crosstab(index=test_data['y_yes'], columns=np.round(predictions.T[0]), rownames=['actuals'], colnames=['predictions'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The results may vary depending on the data cleaning steps you applied before training, but in general you will see that the model is slightly biased towards '0', e.g. people who did subscribe for a term deposit. Still, the results are not bad."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [Optional] Hosting an Endpoint\n",
    "\n",
    "Another method for deploying your model is through endpoints. This part of the workshop is optional, because an endpoint takes around 10 minutes to start (remember, these are endpoints suitable for production workloads). \n",
    "\n",
    "Again, we go back to our hyperparameter tuning job. You can deploy an endpoint straight from the hyperparameter tuner, which automatically selects the best training job to deploy.\n",
    "\n",
    "First we'll need to determine how we pass data into and receive data from our endpoint. Our data is currently stored as NumPy arrays in memory of our notebook instance. To send it in an HTTP POST request, we'll serialize it as a CSV string and then decode the resulting CSV. Therefore, we set the serializer for the endpoint to a CSV serializers provided by the SageMaker library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_predictor = tuner.deploy(\n",
    "    initial_instance_count=1, \n",
    "    instance_type='ml.t2.medium',\n",
    "    serializer=sagemaker.serializers.CSVSerializer()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Navigate to the console to see the effects of running this one line of code. In the *Models* section, you should see that the deploy step trained and saved a final model based on the best hyperparameters found during the tuning process.\n",
    "\n",
    "<img src=\"img/model.PNG\" width=\"800\" />\n",
    "\n",
    "In the *Endpoints* section, you will see that an API endpoint has been created for you. Similarly, its corresponding configuration can be seen in the *Endpoint configurations* section.\n",
    "\n",
    "<img src=\"img/deploy.PNG\" width=\"800\" />\n",
    "\n",
    "Now that we have an API endpoint, we of course want to test it! Let's run our test set through the API endpoint, letting it predict if a customer will subscribe to a term deposit (1) or not (0). We will generate a confusion matrix to see how well the model performs.\n",
    "\n",
    "<div class=\"alert alert-block alert-info\"><b>Note:</b> For inference with CSV format, SageMaker XGBoost requires that the data does NOT include the target variable.</div>\n",
    "\n",
    "Now, we'll use a simple function to:\n",
    "* Loop over our test dataset\n",
    "* Split it into mini-batches of rows\n",
    "* Convert those mini-batches to CSV string payloads (notice, we drop the target variable from our dataset first)\n",
    "* Retrieve mini-batch predictions by invoking the XGBoost endpoint\n",
    "* Collect predictions and convert from the CSV output our model provides into a NumPy array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(data, rows=500):\n",
    "    split_array = np.array_split(data, int(data.shape[0] / float(rows) + 1))\n",
    "    predictions = ''\n",
    "    for array in split_array:\n",
    "        predictions = ','.join([predictions, my_predictor.predict(array).decode('utf-8')])\n",
    "\n",
    "    return np.fromstring(predictions[1:], sep=',')\n",
    "\n",
    "predictions = predict(test_data.drop(['y_yes'], axis=1).to_numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we'll check our confusion matrix to see how well we predicted versus actuals."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.crosstab(index=test_data['y_yes'], columns=np.round(predictions), rownames=['actuals'], colnames=['predictions'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These results should be the same as the results we received from the batch transform step. If you are up for the challenge, see if you can improve these results by playing with the code provided in this notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion <a class=\"anchor\" id=\"conclusion\"></a>\n",
    "\n",
    "In this notebook, you learned how to use SageMaker in an end-to-end machine learning workflow. SageMaker is a powerful tool, with many additional options not covered in this workshop, so we recommend reading the [documentation](https://docs.aws.amazon.com/sagemaker/index.html) to find out more. There are also plenty of [detailed blogs](https://aws.amazon.com/blogs/?filtered-posts.q=sagemaker&filtered-posts.q_operator=AND) which provide advice and ideas on using SageMaker in your projects.\n",
    "\n",
    "**IMPORTANT: Don't forget to clean up the resources created during this workshop! Otherwise, you will continue to incur costs.**\n",
    "Always go through the following steps when you're finished:\n",
    "1. Run the cell below to delete the model, the endpoint configuration, and the endpoint itself.\n",
    "1. Close the notebook through File -> Close and Halt\n",
    "1. Stop the notebook instance by selecting the instance in the *Notebook instances* section of the console, then selecting Actions -> Stop. Stopping the instance means you are no longer paying for the underlying EC2 server, but the notebook information is still persisted in storage (EBS), so you do continue to pay for storage.\n",
    "1. When you are completely finished experimenting with the notebook and you are happy for it to be completely deleted, select the instance in the *Notebook instances* section of the console, then select Actions -> Delete. You will no longer incur any costs for the notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_predictor.delete_model()\n",
    "my_predictor.delete_endpoint(delete_endpoint_config=True)"
   ]
  }
 ],
 "metadata": {
  "instance_type": "ml.t3.medium",
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  },
  "notice": "Copyright 2018 Amazon.com, Inc. or its affiliates. All Rights Reserved.  Licensed under the Apache License, Version 2.0 (the \"License\"). You may not use this file except in compliance with the License. A copy of the License is located at http://aws.amazon.com/apache2.0/ or in the \"license\" file accompanying this file. This file is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License."
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
